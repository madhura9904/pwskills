{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q1. Probability of a smoker given they use health insurance**\n",
    "\n",
    "Weâ€™re given:\n",
    "- \\( P(H) = 0.70 \\) â†’ probability of using health insurance\n",
    "- \\( P(S|H) = 0.40 \\) â†’ probability of being a smoker given use of health insurance\n",
    "\n",
    "Youâ€™re being asked directly:  \n",
    "\\[\n",
    "P(S|H) = 0.40\n",
    "\\]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. Difference: Bernoulli Naive Bayes vs Multinomial Naive Bayes**\n",
    "\n",
    "| Feature                     | **Bernoulli NB**                        | **Multinomial NB**                     |\n",
    "|----------------------------|-----------------------------------------|----------------------------------------|\n",
    "| Input Type                 | Binary features (0 or 1)                | Count features (e.g., word frequency)  |\n",
    "| Use Case                   | Presence/absence of features            | Text classification with word counts   |\n",
    "| Feature distribution       | Bernoulli distribution                  | Multinomial distribution               |\n",
    "| Example Use                | Spam detection with binary features     | Document classification (word counts)  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. How does Bernoulli Naive Bayes handle missing values?**\n",
    "\n",
    "Missing values (e.g., NaNs) need to be handled **before** training by:\n",
    "- Imputation (e.g., fill with 0 or mean)\n",
    "- Dropping rows/columns with missing values\n",
    "\n",
    "Otherwise, `sklearn` will throw an error during training or prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. Can Gaussian Naive Bayes be used for multi-class classification?**\n",
    "\n",
    "\n",
    "`GaussianNB` in `scikit-learn` supports **multi-class classification** out of the box using the **one-vs-rest** strategy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. Assignment â€” Naive Bayes on Spambase Dataset**\n",
    "\n",
    "Letâ€™s break this down:\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Binarize data for BernoulliNB\n",
    "X_binary = Binarizer().fit_transform(X)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"GaussianNB\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Define scoring\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Evaluate\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    if name == \"BernoulliNB\":\n",
    "        scores = cross_validate(model, X_binary, y, cv=10, scoring=scoring)\n",
    "    else:\n",
    "        scores = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
    "    results[name] = {metric: np.mean(scores[f'test_{metric}']) for metric in scoring}\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {score:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š **Results (Hypothetical Sample)**\n",
    "\n",
    "| Model         | Accuracy | Precision | Recall | F1 Score |\n",
    "|---------------|----------|-----------|--------|----------|\n",
    "| BernoulliNB   | 0.88     | 0.87      | 0.85   | 0.86     |\n",
    "| MultinomialNB | 0.91     | 0.90      | 0.89   | 0.89     |\n",
    "| GaussianNB    | 0.84     | 0.81      | 0.83   | 0.82     |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "- **MultinomialNB** usually performs best with **text data** like this because it models **word frequency**.\n",
    "- **BernoulliNB** works reasonably well when you reduce data to binary (word present/absent).\n",
    "- **GaussianNB** assumes a continuous normal distribution, so it's not optimal for count data like this.\n",
    "\n",
    "**Limitations of Naive Bayes:**\n",
    "- Assumes **feature independence**, which is rarely true in real datasets.\n",
    "- Can be **biased** when features are correlated.\n",
    "- GaussianNB performs poorly with **non-normal distributions**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "- **Multinomial Naive Bayes** is most suited for the Spambase dataset due to its frequency-based features.\n",
    "- Bernoulli works decently when binarized.\n",
    "- Gaussian NB is least suited unless the features are continuous and normally distributed.\n",
    "\n",
    "- Try **TF-IDF transformation** on features.\n",
    "- Apply **feature selection** (e.g., chi-square test).\n",
    "- Compare with other classifiers like SVM or Random Forest.\n",
    "- Perform **grid search** for hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want help writing the full report or visualizing the results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
