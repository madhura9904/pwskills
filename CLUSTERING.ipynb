{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNEUDqBsw0wIzfBNq2eJum",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhura9904/pwskills/blob/main/CLUSTERING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBO3aC3xThox"
      },
      "outputs": [],
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?**\n",
        "- A **contingency matrix** (often called a **confusion matrix**) is a table used to describe the performance of a classification model.\n",
        "- Each row represents the **actual class**, and each column represents the **predicted class** (or vice-versa).\n",
        "- It shows counts like:\n",
        "  - **True Positive (TP)**: Correctly predicted positive samples.\n",
        "  - **True Negative (TN)**: Correctly predicted negative samples.\n",
        "  - **False Positive (FP)**: Incorrectly predicted positive samples.\n",
        "  - **False Negative (FN)**: Incorrectly predicted negative samples.\n",
        "- It helps you calculate metrics like **precision**, **recall**, **F1-score**, and **accuracy**.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?**\n",
        "- A **pair confusion matrix** is used especially in **clustering** or **ranking** tasks, where you compare **pairs** of instances.\n",
        "- It shows how pairs of samples are grouped:\n",
        "  - **Same cluster in ground truth and predicted** (True Positive pair),\n",
        "  - **Same in ground truth but different in prediction** (False Negative),\n",
        "  - **Different in ground truth but same in prediction** (False Positive),\n",
        "  - **Different in both** (True Negative).\n",
        "- It is useful when **grouping relationships** matter more than individual class labels, like in **unsupervised learning** (e.g., clustering).\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?**\n",
        "- **Extrinsic evaluation** measures the performance of a model **based on its effect on an external, real-world task**.\n",
        "- In NLP, this means embedding the model into an application (like machine translation, summarization, or sentiment analysis) and measuring success with **task-specific metrics** (e.g., BLEU score for translation).\n",
        "- It's about **practical utility**: Does the model help accomplish a specific task?\n",
        "\n",
        "---\n",
        "\n",
        "**Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?**\n",
        "- **Intrinsic evaluation** measures **internal qualities** of a model, **independent** of a specific downstream task.\n",
        "- Examples:\n",
        "  - Measuring **perplexity** in a language model,\n",
        "  - **Cluster purity** in clustering,\n",
        "  - Word embedding quality by analogy tests (e.g., \"king - man + woman = queen\").\n",
        "- It focuses on how **good the model itself** is, not how useful it is for a bigger application.\n",
        "\n",
        "Difference:\n",
        "- **Intrinsic** = model-centered, task-independent.\n",
        "- **Extrinsic** = task-centered, practical usefulness.\n",
        "\n",
        "---\n",
        "\n",
        "**Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?**\n",
        "- The **confusion matrix** helps you **see exactly where your model is making mistakes**.\n",
        "- **Strengths**:\n",
        "  - High True Positive and True Negative values.\n",
        "- **Weaknesses**:\n",
        "  - High False Positive or False Negative values.\n",
        "- Example:\n",
        "  - If **False Negatives** are high in a cancer detection model, it means **the model is missing actual cancer cases**, which is dangerous.\n",
        "- You can spot **class imbalances**, **biases**, or whether your model is **too aggressive or too cautious**.\n",
        "\n",
        "---\n",
        "\n",
        "**Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?**\n",
        "Some common intrinsic measures:\n",
        "- **Silhouette Score**:\n",
        "  - Measures how similar a sample is to its own cluster compared to other clusters.\n",
        "  - Ranges from -1 to 1; higher is better.\n",
        "- **Davies-Bouldin Index**:\n",
        "  - Lower values indicate better clustering (less overlap, tighter clusters).\n",
        "- **Calinski-Harabasz Index**:\n",
        "  - Higher is better; measures cluster separation.\n",
        "- **Inertia (within-cluster sum of squares)**:\n",
        "  - Lower is better; measures compactness of clusters (used in K-Means).\n",
        "\n",
        "These measures don't require ground truth labels and focus on **cluster structure**.\n",
        "\n",
        "---\n",
        "\n",
        "**Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?**\n",
        "- **Accuracy** can be misleading if:\n",
        "  - The data is **imbalanced** (e.g., 95% of data is class A).\n",
        "  - **Different costs** for false positives and false negatives (e.g., medical diagnosis).\n",
        "- **Solutions**:\n",
        "  - Use **Precision**, **Recall**, and **F1-Score**.\n",
        "  - Look at **ROC curves** and **AUC (Area Under Curve)**.\n",
        "  - Use **confusion matrix** to understand per-class performance.\n",
        "  - Consider **cost-sensitive evaluation** if some errors are worse than others.\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}