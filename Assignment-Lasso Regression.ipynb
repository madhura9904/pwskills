{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ce8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "Lasso regression helps to do feature selection by making the theta of any one feature zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "The main advantage is that we can distinguish and remove the feature that is least correlated with input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01770a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "The coefficients are the error in the regression model and the slope/theta and the bias that we add to the cost function,which is the \n",
    "absolute slope value. So when theta becomes zero we omit the feature that has a null theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "The tuning parameter is lambda.By assigning several values to it through trial and error,we can find the point at which the theta\n",
    "assumes a zero value and perform feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281029eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "Yes, Lasso Regression can be adapted for non-linear regression problems, but it requires feature engineering to introduce non-linear relationships between the predictors and the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "Ridge regression is used to correct any overfitting that may be present in the data,Lasso regression helps to do feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9220a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "Yes.When features are highly correlated, Lasso tends to select one feature from the group and shrink the coefficients of the others to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff91666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "Wecan choose the optimal point through trial and error where the theta reaches zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
